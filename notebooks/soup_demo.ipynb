{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../expo/proxies/working_proxies.txt','r') as f:\n",
    "    proxylist = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['189.203.18.58:3128', '167.114.107.37:80', '', '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "keywords = input('Search for : ')\n",
    "location = input('Location : ')\n",
    "jobs_scraped = 0\n",
    "\n",
    "target_url = base_url+f'keywords={quote(keywords)}&location={quote(location)}'\n",
    "\n",
    "l=[]\n",
    "o={}\n",
    "k=[]\n",
    "\n",
    "headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}\n",
    "\n",
    "print('Job IDs scraped : ')\n",
    "\n",
    "while start != -1:\n",
    "    \n",
    "    res = requests.get(target_url+f'&start={start}')\n",
    "\n",
    "    if len(re.sub('[\\n <!->]','',res.text))>0:\n",
    "\n",
    "        soup=BeautifulSoup(res.text,'html.parser')\n",
    "        alljobs_on_this_page=soup.find_all(\"li\")\n",
    "        \n",
    "        for x in range(0,len(alljobs_on_this_page)):\n",
    "            try : \n",
    "                jobid = alljobs_on_this_page[x].find(\"div\",{\"class\":\"base-card\"}).get('data-entity-urn').split(\":\")[3]\n",
    "                l.append(jobid)\n",
    "            except :\n",
    "                pass\n",
    "    \n",
    "        print(f'{len(l)}')\n",
    "        start += 25\n",
    "    \n",
    "    else :\n",
    "        start = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "job_posting_url='https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}'\n",
    "\n",
    "for j in range(0,len(l)):\n",
    "\n",
    "    resp = requests.get(job_posting_url.format(l[j]))\n",
    "    soup=BeautifulSoup(resp.text,'html.parser')\n",
    "\n",
    "    try:\n",
    "        o[\"company\"]=soup.find(\"div\",{\"class\":\"top-card-layout__card\"}).find(\"a\").find(\"img\").get('alt')\n",
    "    except:\n",
    "        o[\"company\"]=None\n",
    "\n",
    "    try:\n",
    "        o[\"location\"]=soup.find(\"div\",{\"class\":\"topcard__flavor-row\"}).find(\"span\",{'class':'topcard__flavor--bullet'}).text.strip()\n",
    "    except:\n",
    "        o[\"location\"]=None\n",
    "    \n",
    "    try:\n",
    "        o['posted_time_ago']=soup.find_all(\"div\",{\"class\":\"topcard__flavor-row\"})[1].find('span',{'class':'posted-time-ago__text'}).text.strip()\n",
    "    except:\n",
    "        o['posted_time_ago']=None\n",
    "    \n",
    "    try:\n",
    "        o['applicant_number']=soup.find_all(\"div\",{\"class\":\"topcard__flavor-row\"})[1].find('span',{'class':'num-applicants__caption'}).text.strip()\n",
    "    except:\n",
    "        o['applicant_number']=None\n",
    "\n",
    "    try:\n",
    "        o[\"job_title\"]=soup.find(\"div\",{\"class\":\"top-card-layout__entity-info\"}).find(\"a\").text.strip()\n",
    "    except:\n",
    "        o[\"job_title\"]=None\n",
    "\n",
    "    try :\n",
    "        o[\"job_description\"]=soup.find(\"div\",{\"class\":\"description__text description__text--rich\"}).text.strip()\n",
    "    except:\n",
    "        o[\"job_description\"]=None\n",
    "\n",
    "    try:\n",
    "        o[\"level\"]=soup.find(\"ul\",{\"class\":\"description__job-criteria-list\"}).find(\"li\").text.replace(\"Seniority level\",\"\").strip()\n",
    "    except:\n",
    "        o[\"level\"]=None\n",
    "    try:\n",
    "        o[\"employment_type\"]=soup.find(\"ul\",{\"class\":\"description__job-criteria-list\"}).find_all(\"li\")[1].text.replace(\"Employment type\",\"\").strip()\n",
    "    except:\n",
    "        o[\"employment_type\"]=None\n",
    "    try:\n",
    "        o[\"job_function\"]=soup.find(\"ul\",{\"class\":\"description__job-criteria-list\"}).find_all(\"li\")[2].text.replace(\"Job function\",\"\").strip()\n",
    "    except:\n",
    "        o[\"job_function\"]=None\n",
    "    try:\n",
    "        o[\"industries\"]=soup.find(\"ul\",{\"class\":\"description__job-criteria-list\"}).find_all(\"li\")[3].text.replace(\"Industries\",\"\").strip()\n",
    "    except:\n",
    "        o[\"industries\"]=None\n",
    "\n",
    "    k.append(o)\n",
    "    o={}\n",
    "\n",
    "df = pd.DataFrame(k)\n",
    "timestamp = str(datetime.now())\n",
    "df.to_csv(f'./../expo/{keywords}_{location}_{timestamp.split(\".\")[0]}_linkedinjobs.csv', index=False, encoding='utf-8')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=requests.get('https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3821317085')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are on the lookout for a Data Analyst with a specialization in marketing analytics. In this critical role, you will harness the power of tools like Google Data Studio, BigQuery, and Power BI to derive actionable insights that drive our marketing decisions. Your expertise in analyzing complex datasets and turning them into understandable and strategic information will be essential in shaping our marketing strategies. This position is ideal for someone who thrives on data-driven challenges and has a passion for leveraging technology to inform business decisions.What Youâ€™ll Be DoingData Analysis and Interpretation: Utilize tools like Google Data Studio, BigQuery, and Power BI to analyze marketing data and extract meaningful insights. Developing Reporting Dashboards: Create dynamic dashboards and reports that provide real-time insights into marketing performance. Collaborating with Marketing Teams: Work closely with marketing teams to understand their data needs and provide insights that inform strategy and decision-making. Data Integration and Management: Integrate data from various sources to provide a comprehensive view of marketing performance. Performance Tracking: Monitor key performance indicators (KPIs) to measure the effectiveness of marketing campaigns and strategies. Predictive Analysis: Use data to predict trends and market movements, aiding in proactive strategy development. Training and Support: Provide training and support to team members in using data analysis tools and understanding reports. Who You AreProficient in Data Analysis Tools: You have strong experience with tools such as Google Data Studio, BigQuery, and Power BI. Analytical and Insightful: You possess the ability to analyze complex datasets and turn them into actionable insights. Collaborative in Nature: You work well with marketing teams, understanding their needs and communicating data insights effectively. Skilled in Data Integration: You are adept at integrating and managing data from various sources. Detail-Oriented: You have a keen eye for detail and accuracy in data analysis and reporting. Proactive and Predictive: You are capable of conducting predictive analyses to inform future marketing strategies. Excellent Communicator: You can clearly convey data insights and their implications to non-technical team members. What Success Looks Like30 DaysEstablish a comprehensive, organized online database in BigQuery to store all client and marketing data and integrate lead data from CRMs, digital advertising data, customer opt-ins, and purchasing data to provide meaningful insights that inform marketing decisions. Create and maintain data dashboards in Looker Studio for easy data visualization, analysis, and automated reporting. 60 DaysDevelop statistical models for data analysis, including linear regression modeling and predictive lead scoring, that effectively guide marketing actions. 90 DaysAutomated the generation of actionable insights and trends over time from data analysis to inform our marketing strategy.\\n\\n\\n        Show more\\n\\n        \\n\\n\\n        Show less'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup=BeautifulSoup(resp.text,'html.parser')\n",
    "soup.find(\"div\",{\"class\":\"description__text description__text--rich\"}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-02-03 20:07'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(timestamp).split(':')[0]+':'+str(timestamp).split(':')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/tzikos/Desktop/python tasks/linkedin_job_scrape/expo/csv/chef_athens_2024-02-03 21:37_linkedinjobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import concurrent.futures\n",
    "\n",
    "#opens a csv file of proxies and prints out the ones that work with the url in the extract function\n",
    "\n",
    "with open('./../expo/proxies/proxy-list.txt', 'r') as f:\n",
    "    proxylist = [line.strip() for line in f]\n",
    "\n",
    "def extract(proxy):\n",
    "    #this was for when we took a list into the function, without conc futures.\n",
    "    #proxy = random.choice(proxylist)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    try:\n",
    "        #change the url to https://httpbin.org/ip that doesnt block anything\n",
    "        r = requests.get('https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3821317085', headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=2)\n",
    "    except:\n",
    "        pass\n",
    "    return proxy,r.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(extract, proxylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import concurrent.futures\n",
    "\n",
    "#get the list of free proxies\n",
    "def getProxies():\n",
    "    r = requests.get('https://free-proxy-list.net/')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    table = soup.find('tbody')\n",
    "    proxies = []\n",
    "    for row in table:\n",
    "        if row.find_all('td')[4].text =='elite proxy':\n",
    "            proxy = ':'.join([row.find_all('td')[0].text, row.find_all('td')[1].text])\n",
    "            proxies.append(proxy)\n",
    "        else:\n",
    "            pass\n",
    "    return proxies\n",
    "\n",
    "def extract(proxy):\n",
    "    #this was for when we took a list into the function, without conc futures.\n",
    "    #proxy = random.choice(proxylist)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    try:\n",
    "        #change the url to https://httpbin.org/ip that doesnt block anything\n",
    "        r = requests.get('https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3821317085', headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=2).status_code\n",
    "    except :\n",
    "        r = 'failed'\n",
    "    return proxy, r\n",
    "\n",
    "proxylist = getProxies()\n",
    "#print(len(proxylist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = [executor.submit(extract, proxy) for proxy in proxylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_proxies = [result.result()[0] for result in results if result.result()[1]==200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['117.250.3.58:8080', '189.203.18.58:3128']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(proxy):\n",
    "    #this was for when we took a list into the function, without conc futures.\n",
    "    #proxy = random.choice(proxylist)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    try:\n",
    "        r = requests.get('https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?', headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=1)\n",
    "    except requests.ConnectionError as err:\n",
    "        pass\n",
    "    return proxy, r.status_code\n",
    "\n",
    "proxylist = getProxies()\n",
    "#print(len(proxylist))\n",
    "\n",
    "#check them all with futures super quick\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = [executor.submit(extract, proxy) for proxy in proxylist]\n",
    "\n",
    "working_proxies = []\n",
    "for result in results:\n",
    "    try:\n",
    "        working_proxies.append(result.result())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "with open('./expo/proxies/scraped_working_proxies.txt', 'w') as file:\n",
    "    # Write each element to the file followed by a newline character\n",
    "    for wp in working_proxies:\n",
    "        file.write(\"%s\\n\" % wp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
