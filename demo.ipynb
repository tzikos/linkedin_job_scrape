{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\papan\\AppData\\Local\\Temp\\ipykernel_14616\\3848064833.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links are being collected...\n",
      "Collecting links in page 1\n",
      "Collecting links in page 2\n",
      "Collecting links in page 3\n",
      "Collecting links in page 4\n",
      "Collecting links in page 5\n",
      "Collecting links in page 6\n",
      "Collecting links in page 7\n",
      "Collecting links in page 8\n",
      "Collecting links in page 9\n",
      "Collecting links in page 10\n",
      "Collecting links in page 11\n",
      "Collecting links in page 12\n",
      "Collecting links in page 13\n",
      "Collecting links in page 14\n",
      "Collecting links in page 15\n",
      "Collecting links in page 16\n",
      "Collecting links in page 17\n",
      "Collecting links in page 18\n",
      "Collecting links in page 19\n",
      "Collecting links in page 20\n",
      "Collecting links in page 21\n",
      "Collecting links in page 22\n",
      "Collecting links in page 23\n",
      "Collecting links in page 24\n",
      "Collecting links in page 25\n",
      "Collecting links in page 26\n",
      "Collecting links in page 27\n",
      "Collecting links in page 28\n",
      "Collecting links in page 29\n",
      "Collecting links in page 30\n",
      "Collecting links in page 31\n",
      "Collecting links in page 32\n",
      "Collecting links in page 33\n",
      "Collecting links in page 34\n",
      "Collecting links in page 35\n",
      "Collecting links in page 36\n",
      "Collecting links in page 37\n",
      "Collecting links in page 38\n",
      "Collecting links in page 39\n",
      "Collecting links in page 40\n"
     ]
    }
   ],
   "source": [
    "# Driver's path\n",
    "path = 'C:/Users/papan/Downloads/chromedriver_win32'\n",
    "driver = webdriver.Chrome(path)\n",
    "# Maximize Window\n",
    "driver.maximize_window() \n",
    "driver.minimize_window() \n",
    "driver.maximize_window() \n",
    "driver.switch_to.window(driver.current_window_handle)\n",
    "driver.implicitly_wait(10)\n",
    "### Login to dummy linked in account and do the same process because of restriction with authwall login\n",
    "\n",
    "# Enter to the site\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "time.sleep(2)\n",
    "# # Accept cookies\n",
    "driver.find_element('xpath','//*[@id=\"artdeco-global-alert-container\"]/div/section/div/div[2]/button[1]').click()\n",
    "# # User Credentials\n",
    "# Reading txt file where we have our user credentials\n",
    "with open('user_credentials.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    user_credentials = file.readlines()\n",
    "    user_credentials = [line.rstrip() for line in user_credentials]\n",
    "user_name = user_credentials[0] # First line\n",
    "password = user_credentials[1] # Second line\n",
    "driver.find_element('xpath','//*[@id=\"username\"]').send_keys(user_name)\n",
    "driver.find_element('xpath','//*[@id=\"password\"]').send_keys(password)\n",
    "time.sleep(1)\n",
    "# Login button\n",
    "driver.find_element('xpath','//*[@id=\"organic-div\"]/form/div[3]/button').click()\n",
    "driver.implicitly_wait(30)\n",
    "# if driver.find_element('xpath','//*[@id=\"ember7\"]/h2').text == 'Confirm your account information':\n",
    "#     driver.find_element('xpath','//*[@id=\"ember20\"]/button[1]').click()\n",
    "driver.get('https://www.linkedin.com/jobs/search/?currentJobId=3592708886&geoId=104677530&location=Greece')\n",
    "time.sleep(5)\n",
    "# driver.execute_script(\"return window.stop();\")\n",
    "page=1\n",
    "links = []\n",
    "print('Links are being collected...')\n",
    "while True:\n",
    "    \n",
    "    try:\n",
    "        jobs_block = driver.find_element(By.CLASS_NAME,'jobs-search-results-list')\n",
    "        jobs_list = jobs_block.find_elements(By.TAG_NAME,'li')\n",
    "        print(f'Collecting links in page {page}')\n",
    "        for job in jobs_list:\n",
    "            if job == jobs_list[-1]:\n",
    "                all_links = BeautifulSoup(job.get_attribute('innerHTML'),'html').find_all('a',href=True)\n",
    "                for a in all_links:\n",
    "                    if a['href'].startswith(\"/jobs/view\") and a['href'] not in links: \n",
    "                        links.append(a['href'])\n",
    "                    else:\n",
    "                        pass\n",
    "                page += 1\n",
    "                driver.find_element('xpath',f'//button[@aria-label=\"Page {page}\"]').click()\n",
    "                time.sleep(3)\n",
    "                \n",
    "            else:\n",
    "                # scroll down for each job element\n",
    "                driver.execute_script('arguments[0].scrollIntoView();', job)\n",
    "                all_links = BeautifulSoup(job.get_attribute('innerHTML'),'html').find_all('a',href=True)\n",
    "                for a in all_links:\n",
    "                    if a['href'].startswith(\"/jobs/view\") and a['href'] not in links: \n",
    "                        links.append(a['href'])\n",
    "                    else:\n",
    "                        pass\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "links2 = links[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\papan\\AppData\\Local\\Temp\\ipykernel_14616\\3382638812.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting the links and collecting information just started.\n",
      "Scraping the Job Offer 1 DONE.\n",
      "Scraping the Job Offer 2 DONE.\n"
     ]
    }
   ],
   "source": [
    "# Driver's path\n",
    "path = 'C:/Users/papan/Downloads/chromedriver_win32'\n",
    "driver = webdriver.Chrome(path)\n",
    "# Maximize Window\n",
    "driver.maximize_window() \n",
    "driver.minimize_window() \n",
    "driver.maximize_window() \n",
    "driver.switch_to.window(driver.current_window_handle)\n",
    "driver.implicitly_wait(10)\n",
    "### Login to dummy linked in account and do the same process because of restriction with authwall login\n",
    "\n",
    "# Enter to the site\n",
    "driver.get('https://www.linkedin.com/login')\n",
    "time.sleep(2)\n",
    "# # Accept cookies\n",
    "driver.find_element('xpath','//*[@id=\"artdeco-global-alert-container\"]/div/section/div/div[2]/button[1]').click()\n",
    "# # User Credentials\n",
    "# Reading txt file where we have our user credentials\n",
    "with open('user_credentials.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    user_credentials = file.readlines()\n",
    "    user_credentials = [line.rstrip() for line in user_credentials]\n",
    "user_name = user_credentials[0] # First line\n",
    "password = user_credentials[1] # Second line\n",
    "driver.find_element('xpath','//*[@id=\"username\"]').send_keys(user_name)\n",
    "driver.find_element('xpath','//*[@id=\"password\"]').send_keys(password)\n",
    "time.sleep(1)\n",
    "# Login button\n",
    "driver.find_element('xpath','//*[@id=\"organic-div\"]/form/div[3]/button').click()\n",
    "driver.implicitly_wait(30)\n",
    "# Create empty lists to store information\n",
    "job_titles = []\n",
    "company_names = []\n",
    "company_locations = []\n",
    "work_methods = []\n",
    "post_dates = []\n",
    "work_times = [] \n",
    "job_desc = []\n",
    "\n",
    "\n",
    "# Visit each link one by one to scrape the information\n",
    "print('Visiting the links and collecting information just started.')\n",
    "for i in range(len(links2)):\n",
    "    try:\n",
    "        driver.get('https://www.linkedin.com'+links2[i])\n",
    "        time.sleep(2)\n",
    "        # Click See more.\n",
    "        buttons = driver.find_elements(By.CLASS_NAME,\"artdeco-card__actions\")\n",
    "        for button in buttons:\n",
    "            if button.text == 'See more':\n",
    "                button.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Find the general information of the job offer\n",
    "    # Used try, except in case there is some missing information for some of the job offers\n",
    "    contents = driver.find_elements(By.CLASS_NAME, 'p5')\n",
    "    for content in contents:\n",
    "        try:\n",
    "            job_titles.append(content.find_element(By.TAG_NAME, \"h1\").text)\n",
    "        except:\n",
    "            job_titles.append('')  # Append an empty string if element is not found\n",
    "\n",
    "        try:\n",
    "            company_names.append(content.find_element(By.CLASS_NAME, \"jobs-unified-top-card__company-name\").text)\n",
    "        except:\n",
    "            company_names.append('')  # Append an empty string if element is not found\n",
    "\n",
    "        try:\n",
    "            company_locations.append(content.find_element(By.CLASS_NAME, \"jobs-unified-top-card__bullet\").text)\n",
    "        except:\n",
    "            company_locations.append('')  # Append an empty string if element is not found\n",
    "\n",
    "        try:\n",
    "            work_methods.append(content.find_element(By.CLASS_NAME, \"jobs-unified-top-card__workplace-type\").text)\n",
    "        except:\n",
    "            work_methods.append('')  # Append an empty string if element is not found\n",
    "\n",
    "        try:\n",
    "            post_dates.append(content.find_element(By.CLASS_NAME, \"jobs-unified-top-card__posted-date\").text)\n",
    "        except:\n",
    "            post_dates.append('')  # Append an empty string if element is not found\n",
    "\n",
    "        try:\n",
    "            work_times.append(content.find_element(By.CLASS_NAME, \"jobs-unified-top-card__job-insight\").text)\n",
    "        except:\n",
    "            work_times.append('')  # Append an empty string if element is not found\n",
    "\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Scraping the job description\n",
    "    job_description = driver.find_elements(By.CLASS_NAME,'jobs-description__content')\n",
    "    for description in job_description:\n",
    "        job_text = description.find_element(By.CLASS_NAME,\"jobs-box__html-content\").get_attribute('innerHTML')\n",
    "        job_desc.append(str(job_text))\n",
    "        time.sleep(2)  \n",
    "    \n",
    "    print(f'Scraping the Job Offer {i+1} DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Jobs_df = pd.DataFrame({'Title':job_titles,'Company':company_names,'Location':company_locations,'Work Methods':work_methods,'Post Date':post_dates,'Work Times':work_times,'Job Description':job_desc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Work Methods</th>\n",
       "      <th>Post Date</th>\n",
       "      <th>Work Times</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Χειριστής μηχανών παραγωγής και υπεύθυνος βάρδ...</td>\n",
       "      <td>PALAMIDIS SA</td>\n",
       "      <td>Athens, Attiki, Greece</td>\n",
       "      <td>On-site</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>\\n        &lt;h2 class=\"text-heading-large mb4\"&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IT BUSINESS ANALYST</td>\n",
       "      <td>Swatch Group</td>\n",
       "      <td>Athens, Attiki, Greece</td>\n",
       "      <td></td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>Full-time · Associate</td>\n",
       "      <td>\\n        &lt;h2 class=\"text-heading-large mb4\"&gt;\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title       Company  \\\n",
       "0  Χειριστής μηχανών παραγωγής και υπεύθυνος βάρδ...  PALAMIDIS SA   \n",
       "1                                IT BUSINESS ANALYST  Swatch Group   \n",
       "\n",
       "                 Location Work Methods    Post Date             Work Times  \\\n",
       "0  Athens, Attiki, Greece      On-site  3 weeks ago              Full-time   \n",
       "1  Athens, Attiki, Greece                1 week ago  Full-time · Associate   \n",
       "\n",
       "                                     Job Description  \n",
       "0  \\n        <h2 class=\"text-heading-large mb4\">\\...  \n",
       "1  \\n        <h2 class=\"text-heading-large mb4\">\\...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jobs_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
